# Multi-layer-Kernel-Machine

## About

This is a project to construct an efficient multi-layer kernel machine with Monte Carlo methods, using kernel approximation methods. Besides, we add some tricks such as residual learning and data splitting.

## Data

To complete our experiments:

We use: (1) YearPredictionMSD Data Set (regression), in which there are 463,715 train examples and 51,630 test examples. In each example, there are 90 attributes, and an outcome value: the year, ranging from 1922 to 2011. (2) Dry Bean Data set (classification), in which there are 13611 instances and 16 attributes. There are 7 classes of plants (BARBUNYA	BOMBAY	CALI	DERMASON	HOROZ	SEKER	SIRA). (3) OptDigits Data Set (classification), in which there are 3823 train examples and 1797  test examples. In each example, there are 64 = 8 x 8 attributes. (4) California Housing Data set (regression), in which there are 20640 instances and 8 attributes.


## Reference

[1] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.

[2] Thierry Bertin-Mahieux, Daniel P.W. Ellis, Brian Whitman, and Paul Lamere. The Million Song Dataset. In Proceedings of the 12th International Society
for Music Information Retrieval Conference (ISMIR 2011), 2011.

[3] Ali Rahimi and Benjamin Recht. "Random Features for Large-Scale Kernel Machines." Advances in Neural Information Processing Systems. 2008.

[4] Tianshu Huang, Nimit Kalra. "Fast Random Kernelized Features" [https://github.com/qw3rtman/random-feature-maps].
